---
title: "logistic"
author: "Xinran Sun, Haotian Wu, Lin Yang, Shengzhi Luo"
date: "3/17/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

library(tidyverse)
library(caret)
library(ggcorrplot)
library(MASS)
library(pROC)
library(glmnet)
```

## data import and data clean
```{r}
#load the data
breast = read.csv("breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-1, -33) %>% #drop id and NA columns
  mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0))

#check collinearity
corr = breast[2:31] %>% 
  cor()

ggcorrplot(corr, type = "upper", tl.cex = 8)

#remove some highly correlated variables
breast_dat <- breast %>% dplyr::select(-area_se, -perimeter_se, -area_worst, -perimeter_mean, -perimeter_worst, -area_mean, -radius_worst, -concave_points_mean, -texture_worst, -compactness_mean, -concavity_worst)

corr1 = breast_dat[2:20] %>% 
  cor()

ggcorrplot(corr1, type = "upper", tl.cex = 8)

#partition data into training and test data
trainRows <- createDataPartition(y = breast_dat$diagnosis, p = 0.8, list = FALSE)
breast_train <- breast_dat[trainRows, ]
breast_test <-  breast_dat[-trainRows, ]

head(breast_dat, 5)

r = dim(breast_dat)[1] #row number
c = dim(breast_dat)[2] #column number

var_names = names(breast_dat)[-c(1,2)] #variable names
  
standardize = function(col) {
  mean = mean(col)
  sd = sd(col)
  return((col - mean)/sd)
}

stand_df = breast_dat %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize) #standardize

X = stand_df #predictors
y = as.vector(ifelse(breast_dat[,2] == "M", 1, 0))#response
```

```{r}
x_train <- breast_train[2:20] #predictors
y_train <- breast_train[1] #response
x_train_stan <- cbind(rep(1, nrow(x_train)), scale(x_train))

x_test <- breast_test[2:20]
x_test_stan <- cbind(rep(1, nrow(x_test)), scale(x_test))
```


## Full logistic model
```{r}
glm.fit <- glm(diagnosis ~ ., 
               data = breast_dat, 
               subset = trainRows, 
               family = binomial(link = "logit"))
summary(glm.fit)

pred <- predict(glm.fit, newdata = breast_test, type = "response")
y_test <- factor(breast_test$diagnosis)
auc_full <- auc(y_test, pred)
auc_full
```



## Newton-Raphson algorithm

```{r haotian lin}
## logistic stuff 

#logisticstuff = function(dat, betavec){
#  
#  x = dat[, -1] %>% as.matrix()
#  x = cbind(rep(1, nrow(x)), scale(x))
#  y = as.matrix(dat[, 1])
#
#  theta = x %*% betavec
#  p = exp(theta) / (1 + exp(theta))
#  
#  loglik = sum(y * theta - log(1 + exp(theta)))
#
#  grad = t(x) %*% (y - p) # gradient
#  
# # w = p * (1 - p)
# # w = diag(as.vector(w), nrow = nrow(w))
# # print(w)
#  hess = -(t(x) %*% diag(c(p*(1-p))) %*% x) # hessian matrix
#  
#  return(list(loglik = loglik, grad = grad, hess = hess))
#
#}
#
#beta = rep(1, 20) 
#test = logisticstuff(breast_train, betavec = beta)
#test$loglik
```


```{r haotian lin}
#
#newtonraphson = function(dat, func, start, tol = 1e-10, maxiter = 200){
#  i = 0
#  curbeta = start
#  stuff = func(dat, curbeta)
#  res = c(0, stuff$loglik, curbeta)
#  prevloglik = -Inf
#   
#  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
#    i = i + 1
#    prevloglik = stuff$loglik
#    prev = curbeta
#    curbeta = prev - solve(stuff$hess) %*% stuff$grad
#    stuff = func(dat, curbeta)
#
#    #redirection
#    j = 1
#    while (stuff$loglik < prevloglik) {
#      
#  
#      if (!all(eigen(stuff$hess)$values) < 0) {
#      #gamma = max(eigen(stuff$hess)$values)
#      new_hess = stuff$hess - 0.1*diag(20)
#      curbeta = prev - solve(new_hess) %*% stuff$grad
#      }
#      else {
#      j = j/2
#      curbeta = prev - j * solve(stuff$hess) %*% stuff$grad
#      }
#      
#    }
#    
#    stuff = func(dat, curbeta)
#    res = rbind(res, c(i, stuff$loglik, curbeta))
#  }
#  return(res)
#}
#
#res = newtonraphson(breast_train, logisticstuff, beta)
```



## coordinate-wise optimization of a logistic-lasso model


```{r}
#soft threshold
sfxn <- function(beta, lambda) {
  if (abs(beta) > lambda) {
    return(sign(beta) * (abs(beta) - lambda))
  }
  else {
    return(0)
  }
}
```

```{r}
#coordinate-wise optimization function
coordwise_lasso <- function(lambda, x, y, betastart, tol = exp(-10), maxiter = 5000) {
  i <- 0
  n <- length(y)
  pnum <- length(betastart)
  betavec <- betastart
  loglik <- 0
  res <- c(0, loglik, betavec)
  prevloglik <- -Inf
  while (i < maxiter & abs(loglik - prevloglik) > tol & loglik < Inf) {
    i <- i + 1
    prevloglik <- loglik
    for (j in 1:pnum) {
      theta <- x %*% betavec
      p <- exp(theta) / (1 + exp(theta)) #probability of malignant cases
      w <- p*(1-p) #working weights
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- theta + (y - p)/w #working response
      zwoj <- x[, -j] %*% betavec[-j]
      betavec[j] <- sfxn(sum(w*(x[,j])*(z - zwoj)), lambda) / (sum(w*x[,j]*x[,j]))
    }
    theta <- x %*% betavec
    p <- exp(theta) / (1 + exp(theta)) #probability of malignant cases
    w <- p*(1-p) #working weights
    w <- ifelse(abs(w-0) < 1e-10, 1e-10, w)
    z <- theta + (y - p)/w
    loglik <- sum(w*(z - theta)^2) / (2*n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))
  }
  return(res)
}
#coordwise_res <- coordwise_lasso(lambda = 0.006, x_train_stan, y_train, betastart = rep(0, #20))
#coordwise_res[nrow(coordwise_res), ]
```

We need to calculate lambdamax first to define a sequence of lambda. 
```{r}
x.matrix <- scale(x_train) %>% as.matrix()
y.matrix <- as.matrix(y_train)
lambdamax <- max(abs(t(x.matrix) %*% y.matrix)) #/ nrow(y.matrix)
lambda_seq1 <- exp(seq(log(lambdamax), -5, length = 50))
lambda_seq2 <- exp(seq(log(lambdamax), -5, length = 50))
```


```{r}
#a path of solutions
pathwise <- function(x, y, lambda) {
  n <- length(lambda)
  betastart <- rep(0, 20)
  betas <- NULL
  for (i in 1:n) {
    coordwise_res <- coordwise_lasso(lambda = lambda[i],
                                     x = x,
                                     y = y,
                                     betastart = betastart)
    curbeta <- coordwise_res[nrow(coordwise_res), 3:22]
    betastart <- curbeta
    betas <- rbind(betas, c(curbeta))
  }
  return(data.frame(cbind(lambda, betas)))
}

pathwise_sol <- pathwise(x_train_stan, y_train, lambda_seq2)
pathwise_sol %>% knitr::kable()
```


## cross-validation
```{r}
set.seed(2022)

cv = function(data, lambda) {
  n <- nrow(data)
  data <- data[sample(n), ] #shuffle the data
  folds <- cut(seq(1, nrow(data)), breaks = 5, labels = FALSE) #Create 5 equal size folds
 # mse <- data.frame() #a data frame storing mse results
  #mse_lambda <- vector()
  #se <- vector() #a vector storing test errors
  res <- lambda 
  #se <- vector() #a vectro storing test errors
  
    #Perform 5 fold cross validation
  for (i in 1:5) {
    #partition the data into train and test data
    testRows <- which(folds == i, arr.ind = TRUE)
    data_test <- data[testRows, ]
    data_train <- data[-testRows, ]
    x_train <- data_train[2:20]
    x_train_stan <- cbind(rep(1, nrow(x_train)), scale(x_train))
    y_train <- data_train[1]
    x_test <- data_test[2:20]
    #standardized test data
    x_test_stan <- cbind(rep(1, nrow(x_test)), scale(x_test))
    y_test <- data_test %>% mutate(diagnosis = factor(diagnosis))
    y_test <- y_test$diagnosis
    #Use the test and train data partitions to perform lasso
    path_sol <- pathwise(x = x_train_stan,
                         y = y_train,
                         lambda = lambda)
    auc <- vector()
    for (j in 1:length(lambda)) {
      curbeta <- as.numeric(path_sol[j, 2:21])
      theta <- x_test_stan %*% curbeta
      p <- exp(theta) / (1 + exp(theta)) 
      auc[j] <- auc(y_test, p)
      #y.pred <- ifelse(p > 0.5, 1, 0)
      #accuracy[j] <- mean(y.pred == y_test)
    }
    print(auc)
    res <- cbind(res, auc)
    print(res)
  }
  return(res)
    #se[j] <- sqrt(var(error)/5)
  #cv.auc.lambda <- rowMeans(mse)
  #return(cv.auc.lambda)
}

cv_test = cv(data = breast_train, lambda_seq2)

cv_res <- as.data.frame(cv_test) #colnames(c("auc1", "auc2", "auc3", "auc4", "auc5"))
colnames(cv_res) <- c("res", "auc1", "auc2", "auc3", "auc4", "auc5")
cv_lambda <- cv_res[1]
mean_auc <- cv_res %>% dplyr::select(-1) %>% rowMeans()
cv_auc <- cbind(cv_lambda, mean_auc)
maxauc <- max(cv_auc$mean_auc)
bestlambda <- cv_auc[which(cv_auc$mean_auc == maxauc ),]$res
cv_auc %>% 
  ggplot(x = res, y = mean_auc ) +
  geom_line(aes(x = res, y = mean_auc), col = "blue") +
  geom_vline(xintercept = bestlambda, linetype = "dashed", col = "red") +
  labs(title = "Mean AUC vs. Lambda",
       x = "Lambda",
       y = "Mean AUC")
```


## Compare full model and lasso model
```{r}
#corresponding betas of best lambda
lasso_beta <- pathwise_sol[which(pathwise_sol$lambda == bestlambda ),][2:21] %>% as.numeric()

#prediction performance function
predict <- function(x, y, betavec) {
  theta <- x %*% betavec
  p <- exp(theta) / (1 + exp(theta))
  auc <- auc(y, p)
}

auc_lasso <- predict(x_test_stan, y_test, lasso_beta)
auc_lasso

cbind(auc_full, auc_lasso) %>% knitr::kable()

```


```{r}
#coefficients of full and lasso models
glm_beta <- glm.fit$coefficients %>% as.vector()
coefnames <- rownames(coef(summary(glm.fit)))
cbind(coefnames, glm_beta, lasso_beta) %>% knitr::kable()
```

