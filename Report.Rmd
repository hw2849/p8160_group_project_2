---
title: "Project 2: Breast Cancer Diagnosis"
author: "Shengzhi Luo, Xinran Sun, Haotian Wu, Lin Yang"
date: "3/31/2022"
header-includes:
   - \usepackage{amsmath}
output: pdf_document
---
# Objectives

A mammogram is an X-ray image of breast tissue. It can help save lives because it is easier to treat breast cancer in its early stages before the cancer is big enough to detect or cause symptoms. However, a wrong diagnosis can have a negative impact on patients. For example, if there is a false-positive test result, the doctor sees something that looks like cancer but is not. This could result in overtreatment that causes unnecessary side effects on patients. On the other hand, false-negative test result occurs when a doctor misses cancer tissues, which may delay the treatment$^1$. Therefore, building a model that gives an accurate classification of the tissue images is necessary to give proper treatment. In our study, we collected 569 images from both malignant and benign cancer tissues. Our goal is to build a predictive model to facilitate cancer diagnosis.

# Dataset

Our data set consists of 569 rows, with 357 benign and 212 malignant. We denote 0 for benign and 1 for malignant. We also have 30 columns representing the features of the tissue images. They include the mean, standard deviation, and the largest values of the distributions of the following 10 features computed for the cell nuclei:
\begin{itemize}
\item radius (mean of distances from center to points on the perimeter)
\item texture (standard deviation of gray-scale values)
\item perimeter
\item area
\item smoothness (local variation in radius lengths)
\item compactness ($perimeter^2/area$ - 1.0)
\item concavity (severity of concave portions of the contour)
\item concave points (number of concave portions of the contour)
\item symmetry
\item fractal dimension ("coastline approximation" - 1)
\end{itemize}

# EDA

Before building the model, we want have a close look at the dataset. Therefore, we first examine the correlation between variables(**Figure 1**). The squares with dark color in the correlation plot has strong correlation with each other. We can see there is very strong correlation between radius, perimeter and area across mean, standard deviation, and the largest values. We decided to drop some variables with correlation larger than 0.7. The variables we dropped are perimeter_mean, area_mean, compactness_mean, concave_points_mean, perimeter_se, area_se, radius_worst, texture_worst, perimeter_worst, area_worst, and concavity_worst (11 variables).

After that, we built feature plot to analyze the relationship between variables after removing the variables with high correlation (**Figure 2**). From this plot, we can see that there are no strong relationship between variables after removing. We also found that the points for benign tissues are often locate at left-bottom side, which indicates the benign tissues usually have smaller feature values compared to malignant tissues.

We also calculated the mean of each variables to compare values between benign and malignant cases. According to the average values of the mean of each feature, we can find that benign tissues have smaller values compared to malignant tissues, except for fractal dimension. There is no general pattern for the average values of the standard deviations. Based the average values of the largest value of each feature, we can find that benign tissues have smaller largest values compared to malignant tissues.

To compare prediction performance of different models, the dataset is partitioned into the training data (0.8) and the test data (0.2).

# Methods


### Logistic Regression Model

Let \textit{y} be the vector with 569 binary response variable, \textit{X} be the $569 \times 19$ matrix with 19 numerical explanatory variables, and \textit{$\beta$} be the vector with 19 corresponding coefficients. We also have \textit{$\beta_0$} as the intercept.

For our logistic model, the probability of \textit{i}th row be a malignant tissue is given by:
\[P(y_i=1|X_i) = \frac{e^{\beta_0+\beta X_i}}{1+e^{\beta_0+\beta X_i}}.\]
For likelihood function is:
\[L(\beta_0,\beta) = \prod_{i=1}^n [(\frac{e^{\beta_0+\beta X_i}}{1+e^{\beta_0+\beta X_i}})^{y_i}(\frac{1}{1+e^{\beta_0+\beta X_i}})^{1-y_i}].\]
Maximizing the likelihood is equivalent to maximizing the log likelihood:
\[f(\beta_0,\beta) = \sum_{i=1}^n [y_i(\beta_0+\beta X_i)-\log(1+e^{\beta_0+\beta X_i})].\]
The gradient of this function is:
\[\nabla f(\beta_0,\beta)= \begin{pmatrix}
\sum_{i=1}^n y_i-p_i\\
\sum_{i=1}^n X_1(y_i-p_i)\\
...\\
\sum_{i=1}^n X_n(y_i-p_i)
\end{pmatrix} = X^T(y_i-p_i)\]
where $p_i = P(y_i=1|X_i)$ as mentioned in previous probability function.

The Hessian is given by
\[\nabla^2 f(\beta_0,\beta) = -X^TWX\]
where $W = p_i(1-p_i)$.

### Newton-Raphson Algorithm


Newton-Raphson algorithm is a method to search for solutions to the system of equations $\nabla f(\beta_0,\beta)=0$. 
At each step, given the current point $\boldsymbol{\beta}_0$, the gradient $\nabla f(\beta_0,\beta)$ for $\boldsymbol{\beta}$ near $\boldsymbol{\beta}_0$ may be approximated by 

$$
\nabla f(\beta_0,\beta)+\nabla^2 f(\beta_0,\beta)
\left(\boldsymbol{\beta}-\boldsymbol{\beta}_{0}\right)
$$

The next step in the algorithm is determined by solving the system of linear equations

$$
\nabla f(\beta_0,\beta)+\nabla^2 f(\beta_0,\beta)\left(\boldsymbol{\beta}-\boldsymbol{\beta}_{0}\right)=\mathbf{0}
$$
and the next “current point” is set to be the solution, which is a function of $\beta_0$:

$$
\boldsymbol{\beta}_{1}=\boldsymbol{\beta}_{0}-[\nabla^2 f(\beta_0,\beta)]^{-1} \nabla f(\beta_0,\beta)
$$

The ith step is given by a function of $\beta_{i-1}$:

$$
\boldsymbol{\beta}_{i}=\boldsymbol{\beta}_{i-1}-[\nabla^2 f(\beta_{i-1},\beta)]^{-1} \nabla f(\beta_{i-1},\beta)
$$

The Newton Raphson algorithm iterates through i beta values until the log-likelihood loss has converged. For this project, we uesd an additional half-stepping modiﬁcation to the algorithm to control the number of iteration steps. 


### Path-wise Coordinate-wise Optimization Algorithm for Logistic-LASSO Model
To obtain a path of coefficients for a descending sequence of tuning parameter $\lambda$, we need to develop a coordinate-wise descent algorithm estimating coefficients for a specific lambda.
The logistic-LASSO can be written as a penalized weighted least-squares problem:

$$
\min _{\left(\beta_{0}, \boldsymbol{\beta}_{1}\right)} L\left(\beta_{0}, \boldsymbol{\beta}_{1}, \lambda\right)=\left\{-\ell\left(\beta_{0}, \boldsymbol{\beta}_{1}\right)+\lambda \sum_{j=0}^{p}\left|\beta_{j}\right|\right\}
$$

When there are a large number of parameters, i.e., p is large, a coordinate-wise descent algorithm is required to optimize coefficients. The objective function is:

$$
f\left(\beta_{j}\right)=\frac{1}{2} \sum_{i=1}^{n}\left(y_{i}-\sum_{k \neq j} x_{i, k} \widetilde{\beta}_{k}-x_{i, j} \beta_{j}\right)^{2}+\gamma \sum_{k \neq j}\left|\widetilde{\beta}_{k}\right|+\gamma\left|\beta_{j}\right|
$$

Minimizing $f\left(\beta_{j}\right)$ w.r.t $\beta_{j}$ while having $\widetilde{\beta}_{k}$ fixed, we have weighted updates to update one coefficient at a time iteratively until the log-likelihood converges:

$$
\widetilde{\beta}_{j}(\lambda) \leftarrow \frac{S\left(\sum_{i} w_{i} x_{i, j}\left(y_{i}-\tilde{y}_{i}^{(-j)}\right), \lambda\right)}{\sum_{i} w_{i} x_{i, j}^{2}}
$$

where $\tilde{y}_{i}^{(-j)}=\sum_{k \neq j} x_{i, k} \widetilde{\beta}_{k}$. 

If we apply Taylor expansion to the log-likelihood around "current estimates" $\left(\widetilde{\beta}_{0}, \tilde{\beta}_{1}\right)$, we have a quadratic approximation function $f\left(\beta_{0}, \boldsymbol{\beta}_{1}\right)$ to the log-likelihood:

$$
f\left(\beta_{0}, \boldsymbol{\beta}_{1}\right) \approx \ell\left(\beta_{0}, \boldsymbol{\beta}_{1}\right)=-\frac{1}{2 n} \sum_{i=1}^{n} w_{i}\left(z_{i}-\beta_{0}-\mathbf{x}_{i}^{T} \boldsymbol{\beta}_{1}\right)^{2}+C\left(\widetilde{\beta}_{0}, \widetilde{\boldsymbol{\beta}}_{1}\right)
$$
where 
$$
z_{i}=\widetilde{\beta}_{0}+\mathbf{x}_{i}^{T} \widetilde{\boldsymbol{\beta}}_{1}+\frac{y_{i}-\widetilde{p}_{i}\left(\mathbf{x}_{i}\right)}{\widetilde{p}_{i}\left(\mathbf{x}_{i}\right)\left(1-\widetilde{p}_{i}\left(\mathbf{x}_{i}\right)\right)}
$$

$$
w_{i}=\widetilde{p}_{i}\left(\mathbf{x}_{i}\right)\left(1-\widetilde{p}_{i}\left(\mathbf{x}_{i}\right)\right)
$$

$$
\widetilde{p}_{i}=\frac{\exp \left(\widetilde{\beta}_{0}+\mathbf{x}_{i}^{T} \widetilde{\boldsymbol{\beta}}\right)}{1+\exp \left(\widetilde{\beta}_{0}+\mathbf{x}_{i}^{T} \widetilde{\boldsymbol{\beta}}_{1}\right)}
$$
$w_{i}$ is the working weight, $z_{i}$ is the working response, $p_{i}$ is the probability of malignant case estimated at current coefficients. This quadratic approximation function is used in the coordinate-wise descent algorithm.\
\
We then can develop a path-wise coordinate-wise optimization algorithm to get a path of solutions for a descending sequence of $\lambda$.

\begin{itemize}
\item Step 1: Find the smallest value $\lambda$ for which all the estimated $\beta$ are 0, defined as $\lambda_{max}$.
\item Step 2: Define a fine sequence $\lambda_{max} \ge \lambda_1 \ge ... \lambda_{min} \ge 0$.
\item Step 3: To estimate coefficients of the current $\lambda_{k+1}$, implement coordinate descent algorithm using the computed coefficients of the previous $\lambda_{k}$ (warm start) as coefficient start values. ($\lambda_{k+1}<\lambda_k$)
\end{itemize}

### Cross Validation

To select the best $\lambda$ for the optimal model, a 5-fold cross-validation is performed (**Figure 3**). 

\begin{itemize}
\item Step 1: Shuffle the original dataset randomly.
\item Step 2: Split the shuffled dataset into 5 even groups.
\item Step 3: Take one group as the test set and the remaining groups as the training set. Implement the path-wise coordinate-wise optimization algorithm based on the training data, and then calculate AUC scores for each $\lambda$ using the test data. 
\item Step 4: Repeat this procedure until each of the 5 groups has been treated as test data, and mean AUC for each $\lambda$ is computed$^2$. 
\end{itemize}


# Results

The coefficients of the full logistic regression model based on the training data using **glm()** are shown in **Table 1**. 5 predictors are found to be significant with p values less than 0.05, including **texture_mean**, **concavity_mean**, **radius_se**, **symmetry_worst**. For the logistic-LASSO model, the $\lambda_{max}$ is calculated to be 175.62, we then defined a descending sequence of 50 $\lambda$ between $\lambda_{max}$ and $e^{-5}$. Since the outcome is a binary variable, AUC is used as the evaluation metric to compare models. Through the 5-fold cross-validation, the best $\lambda$ is found to be $0.981$ with an average AUC of $0.997$ (**Figure 4**). From the plot of the path solutions, we can see that beta coefficients start to diverge at log(0.981) (**Figure 5**). The coefficients for the best $\lambda$ are shown in **Table 2**. As expected, the optimal logistic-LASSO model shrinks some coefficients to 0. 9 of 19 predictors remain in the LASSO model: **radius_mean**, **texture_mean**, **concavity_mean**, **radius_se**, **compactness_se**, **fractal_dimension_se**, **smoothness_worst**, **concave_points_worst**, and **symmetry_worst**.

AUC scores of the full logistic regression model and the optimal logistic-LASSO model are computed based on the test data to compare prediction performance of the two models. The full model's AUC (0.977) is found to be less than the LASSO model's AUC (0.996), thus the optimal LASSO model slightly outperforms the full model. 


# Conclusions

### Findings

The primary goal of our project is to build a model to predict whether a breast tissue sample is benign or malignant. After performing the exploratory data analysis, we drop 11 highly correlated variables, and 19 variables are used to fit a full logistic regression model. A Newton-Raphson algorithm is developed to estimate coefficients of the full model. We also compare the full model with a logistic-LASSO model whose coefficients are estimated by a path-wise coordinate-wise optimization algorithm. The optimal LASSO model with the best $\lambda$ is selected according to a 5-fold CV, and the optimal LASSO model is found to slightly outperform the full logistic model. Based on the optimal LASSO model, we realize some implications on breast cancer diagnosis. For example, breast tissue samples with higher mean radius tend to indicate malignant cases. On the other hand, the ones with lower compactness standard deviation tend to indicate benign cases. 

### Limitations

We found that Newton-Raphson algorithm is unstable during our work procedure. First, the convergence is not guaranteed, and it depends on the choice of starting values. If we set initial betas at large values, the algorithm would not work. Therefore, we have to carefully choose relatively small starting values. In addition, the number of lambdas in the lambda sequence is limited due to intensive computation of cross validation, thus we defined a sequence of 50 lambdas in the path-wise coordinate-wise optimization algorithm. If we include more lambdas, the selection of the best lambda would be more accurate and close to the truth.  


